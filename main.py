import os
import requests
import random
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

# --- –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è ---
TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL = os.getenv("CHANNEL_USERNAME")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
UNSPLASH_KEY = os.getenv("UNSPLASH_ACCESS_KEY")

# --- –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º ---
USED_LINKS_FILE = "used_links.txt"
USED_IMAGES_FILE = "used_images.txt"

# --- RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ ---
RSS_FEEDS = [
    "https://habr.com/ru/rss/flows/develop/all/?fl=ru",
    "https://dev.to/feed",
    "https://medium.com/feed/tag/programming",
    "https://www.blendernation.com/feed/",
    "https://towardsdatascience.com/feed",
    "https://godotengine.org/rss.xml"
]

# --- –§–∏–ª—å—Ç—Ä –ø–æ–ª–∏—Ç–∏–∫–∏ ---
def is_political(text):
    banned = ["—É–∫—Ä–∞–∏–Ω–∞", "–¥–æ–Ω–±–∞—Å—Å", "–∑–µ–ª–µ–Ω—Å–∫–∏–π", "—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è", "—Å–∞–Ω–∫—Ü–∏–∏", "–º–æ–±–∏–ª–∏–∑–∞—Ü–∏—è"]
    return any(word in text.lower() for word in banned)

# --- –ò—Å—Ç–æ—Ä–∏—è –ø—É–±–ª–∏–∫–∞—Ü–∏–π ---
def is_posted(link):
    if not os.path.exists(USED_LINKS_FILE):
        return False
    with open(USED_LINKS_FILE, "r", encoding="utf-8") as f:
        return link in f.read()

def mark_posted(link):
    with open(USED_LINKS_FILE, "a", encoding="utf-8") as f:
        f.write(link + "\n")

# --- –†–∞–±–æ—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ ---
def extract_image_url(entry):
    soup = BeautifulSoup(entry.get("summary", ""), "html.parser")
    img_tag = soup.find("img")
    if img_tag and img_tag.get("src"):
        return img_tag.get("src")
    return None

def is_image_used(url):
    if not os.path.exists(USED_IMAGES_FILE):
        return False
    with open(USED_IMAGES_FILE, "r", encoding="utf-8") as f:
        return url in f.read()

def mark_image_used(url):
    with open(USED_IMAGES_FILE, "a", encoding="utf-8") as f:
        f.write(url + "\n")

def get_image_url(entry):
    url = extract_image_url(entry)
    if url and not is_image_used(url):
        mark_image_used(url)
        return url
    return None

# --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞ —á–µ—Ä–µ–∑ gen-api.ru (Gemini) ---
def generate_post_text(title, summary, source, link):
    prompt = f"""
–¢—ã ‚Äî –∞–≤—Ç–æ—Ä Telegram-–∫–∞–Ω–∞–ª–∞ –æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏.

–°–æ—Å—Ç–∞–≤—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –ø–æ—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –°—Ç–∏–ª—å ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –ø–æ–Ω—è—Ç–Ω—ã–π –¥–∞–∂–µ –Ω–æ–≤–∏—á–∫—É. –ú–∏–Ω–∏–º—É–º 8 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

–§–æ—Ä–º–∞—Ç:
üöÄ <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫</b>  
üìÖ –î–∞—Ç–∞ + –∏—Å—Ç–æ—á–Ω–∏–∫  
üîπ –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç ‚Äî –ø–æ–Ω—è—Ç–Ω—ã–π, –±–µ–∑ –ª–∏—à–Ω–µ–π –≤–æ–¥—ã  
ü§î –í–æ–ø—Ä–æ—Å –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è  
<a href="{link}">üìñ –ß–∏—Ç–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ</a>

–ù–æ–≤–æ—Å—Ç—å:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–û–ø–∏—Å–∞–Ω–∏–µ: {summary}
–ò—Å—Ç–æ—á–Ω–∏–∫: {source}
–°—Å—ã–ª–∫–∞: {link}
"""

    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }

    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7
    }

    try:
        res = requests.post("https://api.deepseek.com/v1/chat/completions", headers=headers, json=data)
        result = res.json()

        if "choices" in result:
            return result["choices"][0]["message"]["content"]
        else:
            print("‚ö†Ô∏è DeepSeek –Ω–µ –≤–µ—Ä–Ω—É–ª 'choices':", result)
            return None
    except Exception as e:
        print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ DeepSeek:", e)
        return None

# --- –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –Ω–æ–≤–æ—Å—Ç–∏ ---
def get_news():
    for _ in range(10):
        feed = feedparser.parse(random.choice(RSS_FEEDS))
        for entry in feed.entries:
            title = entry.get("title", "")
            summary = entry.get("summary", "")
            link = entry.get("link", "")
            source = feed.feed.get("title", "–ò—Å—Ç–æ—á–Ω–∏–∫")

            if is_posted(link):
                continue
            if is_political(title + summary):
                continue
            if not summary or len(summary) < 40:
                continue

            return {
                "title": title,
                "summary": summary,
                "link": link,
                "source": source,
                "entry": entry
            }
    return None

# --- –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram ---
def post_to_telegram(text, image_url=None):
    if image_url:
        url = f"https://api.telegram.org/bot{TOKEN}/sendPhoto"
        data = {
            "chat_id": CHANNEL,
            "photo": image_url,
            "caption": text,
            "parse_mode": "HTML"
        }
    else:
        url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
        data = {
            "chat_id": CHANNEL,
            "text": text,
            "parse_mode": "HTML"
        }

    requests.post(url, data=data)

# --- –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ ---
def main():
    attempts = 0
    while attempts < 5:
        news = get_news()
        if not news:
            print("üòê –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            return

        print("üì∞ –ù–æ–≤–æ—Å—Ç—å:", news["title"])

        post_text = generate_post_text(news["title"], news["summary"], news["source"], news["link"])

        if not post_text or len(post_text.split()) < 20:
            print("‚ùå –°–ª–∞–±—ã–π —Ç–µ–∫—Å—Ç, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –Ω–æ–≤–æ—Å—Ç—å...")
            attempts += 1
            continue

        image_url = get_image_url(news["entry"])
        post_to_telegram(post_text, image_url)
        mark_posted(news["link"])
        print("‚úÖ –ü–æ—Å—Ç –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω!")
        break

    if attempts == 5:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–æ—Å—Ç–æ–π–Ω—ã–π –ø–æ—Å—Ç.")

if __name__ == "__main__":
    main()

